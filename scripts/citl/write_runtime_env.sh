#!/usr/bin/env bash
set -euo pipefail

ROOT="$(cd "$(dirname "${BASH_SOURCE[0]}")/../.." && pwd)"
OUT="$ROOT/config/runtime.env"

# Choose first custom model name (from Modelfiles)
pick_custom_model() {
  find "$ROOT" -type f -name Modelfile \
    -not -path "*/.git/*" \
    -not -path "*/.venv/*" \
    -print0 | while IFS= read -r -d '' mf; do
      dir="$(dirname "$mf")"
      name="$(basename "$dir")"
      override="$(grep -E '^\s*#\s*NAME:\s*' "$mf" | head -n1 | sed -E 's/^\s*#\s*NAME:\s*//')"
      [[ -n "${override:-}" ]] && name="$override"
      echo "$name"
      return 0
    done
}

CHAT_MODEL="$(pick_custom_model || true)"
: "${CHAT_MODEL:=}"

# Embed model heuristic (override by exporting CITL_EMBED_MODEL before running)
EMBED_MODEL="${CITL_EMBED_MODEL:-nomic-embed-text}"

# Default Ollama host (local)
OLLAMA_HOST="${OLLAMA_HOST:-http://127.0.0.1:11434}"

# If no custom model exists, leave blank and let app default
mkdir -p "$(dirname "$OUT")"
cat > "$OUT" <<ENV
# Auto-generated by scripts/citl/write_runtime_env.sh
export OLLAMA_HOST="$OLLAMA_HOST"

# Common model var names used in apps (we set multiple to be safe)
export CITL_CHAT_MODEL="${CHAT_MODEL}"
export OLLAMA_MODEL="${CHAT_MODEL}"
export CHAT_MODEL="${CHAT_MODEL}"
export MODEL="${CHAT_MODEL}"

export CITL_EMBED_MODEL="${EMBED_MODEL}"
export OLLAMA_EMBED_MODEL="${EMBED_MODEL}"
export EMBED_MODEL="${EMBED_MODEL}"

# Optional: set this if you want a specific Streamlit entry file
# export CITL_STREAMLIT_ENTRYPOINT="app.py"
ENV

echo "[runtime] wrote: $OUT"
sed -n '1,120p' "$OUT"
